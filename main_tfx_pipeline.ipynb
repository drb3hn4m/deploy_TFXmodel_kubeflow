{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade \"tfx[kfp]<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# docs_infra: no_execute\n",
    "import sys\n",
    "if not 'google.colab' in sys.modules:\n",
    "  # Automatically restart kernel after installs\n",
    "  import IPython\n",
    "  app = IPython.Application.instance()\n",
    "  app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))\n",
    "\n",
    "# You may see a WARNING issued. You can safely ignore this until the TFX and KFP versions are displayed in the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://som-k8s-gcs/pipeline_root/penguin-vertex-pipelines\n",
      "MODULE_ROOT: gs://som-k8s-gcs/pipeline_module/penguin-vertex-pipelines\n",
      "DATA_ROOT: gs://som-k8s-gcs/data/penguin-vertex-pipelines\n",
      "SERVING_MODEL_DIR: gs://som-k8s-gcs/serving_model/penguin-vertex-pipelines\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'som-k8s'    \n",
    "GOOGLE_CLOUD_REGION = 'australia-southeast1'     \n",
    "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-gcs'\n",
    "PIPELINE_NAME = 'penguin-vertex-pipelines'\n",
    "_trainer_module_file = 'penguin_trainer.py'\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = f'gs://{GCS_BUCKET_NAME}/pipeline_root/{PIPELINE_NAME}'\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = f'gs://{GCS_BUCKET_NAME}/pipeline_module/{PIPELINE_NAME}'\n",
    "# Paths for input data.\n",
    "DATA_ROOT = f'gs://{GCS_BUCKET_NAME}/data/{PIPELINE_NAME}'\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = f'gs://{GCS_BUCKET_NAME}/serving_model/{PIPELINE_NAME}'\n",
    "print(f'PIPELINE_ROOT: {PIPELINE_ROOT}')\n",
    "print(f'MODULE_ROOT: {MODULE_ROOT}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT}')\n",
    "print(f'SERVING_MODEL_DIR: {SERVING_MODEL_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Creating gs://som-k8s-gcs/...\n",
      "Copying gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 25.0 KiB/ 25.0 KiB]                                                \n",
      "Operation completed over 1 objects/25.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {GOOGLE_CLOUD_PROJECT}\n",
    "!gsutil mb 'gs://'{GCS_BUCKET_NAME}\n",
    "!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing penguin_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _make_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://penguin_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, use_gpu: bool\n",
    "                     ) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying penguin_trainer.py -> build/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing to /var/tmp/tmpo4tyf49y\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/penguin_trainer.py -> /var/tmp/tmpo4tyf49y\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /var/tmp/tmpo4tyf49y/tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpo4tyf49y/tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpy0nb1owq/tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9-py3-none-any.whl' and adding '/var/tmp/tmpo4tyf49y' to it\n",
      "adding 'penguin_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+afa8d5dfaf2942b08385545b5e095850331200a5d7a669b1791b3e7efcc5fdd9.dist-info/RECORD'\n",
      "removing /var/tmp/tmpo4tyf49y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/australia-southeast1/pipelines/runs/penguin-vertex-pipelines-20240620001654?project=41931926481\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/41931926481/locations/australia-southeast1/pipelineJobs/penguin-vertex-pipelines-20240620001654\n"
     ]
    }
   ],
   "source": [
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n",
    "# slightly modified because we don't need `metadata_path` argument.\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, use_gpu: bool\n",
    "                     ) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "    # This dictionary will be passed as `CustomJobSpec`.\n",
    "    vertex_job_spec = {\n",
    "        'project': project_id,\n",
    "        'worker_pool_specs': [{\n",
    "            'machine_spec': {\n",
    "                'machine_type': 'n1-standard-4',\n",
    "            },\n",
    "            'replica_count': 1,\n",
    "            'container_spec': {\n",
    "                'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "            },\n",
    "        }],\n",
    "    }\n",
    "    if use_gpu:\n",
    "        # See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype\n",
    "        # for available machine types.\n",
    "        vertex_job_spec['worker_pool_specs'][0]['machine_spec'].update({\n",
    "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
    "            'accelerator_count': 1\n",
    "        })\n",
    "\n",
    "    # Uses user-provided Python function that trains a model. We need to specify a Trainer for GCP with related configs.\n",
    "    trainer =  tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
    "        custom_config={\n",
    "            tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "                True,\n",
    "            tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "                region,\n",
    "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "                vertex_job_spec,\n",
    "            'use_gpu':\n",
    "                use_gpu,\n",
    "        })\n",
    "\n",
    "        # Configuration for pusher.\n",
    "    vertex_serving_spec = {\n",
    "            'project_id': project_id,\n",
    "            'endpoint_name': endpoint_name,\n",
    "            'machine_type': 'n1-standard-4',\n",
    "        }\n",
    "    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "    if use_gpu:\n",
    "        vertex_serving_spec.update({\n",
    "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
    "            'accelerator_count': 1\n",
    "        })\n",
    "        serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-6:latest'\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "      })\n",
    "\n",
    "    # Following three components will be included in the pipeline.\n",
    "    components = [\n",
    "        example_gen,\n",
    "        trainer,\n",
    "        pusher,\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components)\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n",
    "\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        # We will use CPUs only for now.\n",
    "        use_gpu=False))\n",
    "\n",
    "# submit The generated definition file using kfp client.\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.run(sync=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
